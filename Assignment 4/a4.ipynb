{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOquqKzb2Nj4",
        "colab_type": "text"
      },
      "source": [
        "# A4: Visual Question Answering\n",
        "\n",
        "In this assignment, you'll gain experience working with multi-input models. You'll develop a model that takes a question and an image as input, and returns a yes/no answer in response. You'll also gain experience working with larger datasets.\n",
        "\n",
        "You are welcome to implement your model using either Model Subclassing or the Functional API. I suggest using the Functional API - it's worth gaining experience with, so you're familiar with it down the road. \n",
        "\n",
        "In either case, you will likely spend more time preparing and preprocessing the dataset, than you will developing your model.\n",
        "\n",
        "## Your starter dataset\n",
        "\n",
        "The [VQA dataset](https://visualqa.org/download.html) includes images and JSON containing the questions and answers. To save you time parsing the JSON (and various other preprocessing work), I've prepared a subset of the dataset in an easier to read CSV file. \n",
        "\n",
        "You should download your starter dataset from [here](https://storage.cloud.google.com/applied-dl/mini-vqa/starter.csv). \n",
        "\n",
        "Your starter CSV has three columns, giving the question, answer, and image.\n",
        "\n",
        "question|answer|image\n",
        ":-----:|:-----:|:-----:\n",
        "is the sky blue|yes|COCO\\_train2014\\_000000393221.jpg\n",
        "is there snow on the mountains|yes|COCO\\_train2014\\_000000393221.jpg\n",
        "is the window open|no|COCO\\_train2014\\_000000393223.jpg\n",
        "\n",
        "### Notes on how this was prepared.\n",
        "* The starter dataset contains all the \"yes\" / \"no\" questions from the VQA v2 training annotations. \n",
        "* The answers are the most frequent one to the question (ties broken randomly). \n",
        "* The text has been cleaned (lowercased, punctuation removed). \n",
        "* Each image has at least two associated questions, some have more.\n",
        "* There are 140,637 questions, and 42,985 images.\n",
        "* The image filenames correspond to the [COCO 2014 training images](http://images.cocodataset.org/zips/train2014.zip). Note this file is about 11GB. You will need to download this later.\n",
        "\n",
        "### A note on downloading the COCO training images\n",
        "\n",
        "The training images are large, and you may not want to download them repeatedly in Colab (for example, if your instance disconnects). Continue reading the assignment below for tips before downloading them. \n",
        "\n",
        "In general, your workflow for this assignment will be to:\n",
        "\n",
        "Part 1: \n",
        "- Write a separate notebook to download the training images. \n",
        "- Preprocess the images referenced in the starter dataset (e.g. by resizing them to 299x299). Disgard the other images. \n",
        "- Create a zip containing only the resized images from the starter dataset (it should be about 500MB) and upload that to web.\n",
        "- To host a zip on the web, you can use either your Columbia account, or any Cloud provider (for example, [Cloud storage](https://cloud.google.com/storage/docs/creating-buckets)). Do not pay for a Cloud solution for this class (GCP, for example, has a free tier you can use).\n",
        "\n",
        "Part 2:\n",
        "- Write a separate notebook to train your model. At the start of this notebook, download your 500MB zip containing the preprocessed training images using a command like ```!wget``` or the code shown below.\n",
        "\n",
        "### A note on creating and training your model\n",
        "\n",
        "You can find an example VQA model at the bottom of this [guide](https://keras.io/getting-started/functional-api-guide/). Note: you should start with a smaller model (in terms of vocabulary size,sequence length, and other parameters). \n",
        "\n",
        "Other changes: \n",
        "* Instead of training a CNN from scratch, you will use a pretrained model (Inception-V3) and cache activations to disk. \n",
        "* Your model will take a while to train. It is important that you save a [checkpoint](https://www.tensorflow.org/guide/keras/save_and_serialize) to disk after each epoch (or every several epochs). Before training your model, check for existing checkpoints, and load the latest one.\n",
        "\n",
        "Training tips:\n",
        "\n",
        "* When developing a complex model, it is important to fit on a single batch first (repeatedly train on the same batch of 32 examples and labels). Verify your model can successfully memorize this data (the loss should go to zero). \n",
        "\n",
        "* Next, write code to make predictions on each of these images, and to display the image, question, and answer. Make sure the results are sensible. \n",
        "\n",
        "* As always, start with a simple model, get that working end-to-end, then gradually scale up.\n",
        "\n",
        "* Code defensively and get to know your data. There will be several preprocessing steps (resizing images, caching actiations, tokenization, padding, etc). After each one, write a short block of code to verify the output is correct.\n",
        "\n",
        "## Starter code\n",
        "\n",
        "The starter code below contains various blocks you can use for your two notebooks (the first to download and preprocess the data, the second to train your model).\n",
        "\n",
        "## Submission instructions\n",
        "\n",
        "Please submit this assignment on CourseWorks by uploading one or more Jupyter notebooks that include saved output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWV2Ob0unYLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install Pillow # used to resize images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9thHjd3JomO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBeg8kixJ7V2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhTJlvf_o9vF",
        "colab_type": "text"
      },
      "source": [
        "## Notebook one: Download and preprocess the COCO training images\n",
        "Create a new notebook for this part of the assignment.\n",
        "\n",
        "In this section, you will download your starter CSV and the COCO training images. Preprocess the +/- 40,000 images from the starter CSV (by resizing them to 299x299 using Pillow), and create a zip file called \"thumbnails.zip\". Your zip should be around 500MB. \n",
        "\n",
        "Finally, upload your \"thumbnails.zip\" to the Cloud, so you can easily download it in your second notebook.\n",
        "\n",
        "### Tips\n",
        "\n",
        "I recommend downloading the COCO images with curl, so you can resume your download if it is [interrupted](https://stackoverflow.com/questions/19728930/how-to-resume-interrupted-download-automatically-in-curl) or stalls.\n",
        "\n",
        "In Colab (or on Linux / Mac):\n",
        "\n",
        "```\n",
        "$ curl -L -O your_url\n",
        "```\n",
        "\n",
        "You can download these images in Colab, but you may need to be mindful of disk usage. As a tip to save disk space, after unzipping them, you may delete any images from the zip not referenced in the starter code.\n",
        "\n",
        "After you've downloaded them with curl, here's a command  you can use to unzip them:\n",
        "\n",
        "```\n",
        "$ unzip -q from_zip -d to_folder\n",
        "```\n",
        "\n",
        "Also note: there are so many images that opening the folder in the Colab will be slow. Instead of opening the folder with the UI, you can use these shell commands to explore it:\n",
        "\n",
        "```\n",
        "$ ls -lha\n",
        "$ du -sh\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77VYYVBPu5Dx",
        "colab_type": "text"
      },
      "source": [
        "### Download the starter CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS7gkk-pHZZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Starter code to download small files\n",
        "# Use this to download your starter CSV in notebook one and two.\n",
        "# Also use it to download your thumbnails.zip in notebook two.\n",
        "def download_if_missing(url, target, extract=True):\n",
        "  if os.path.exists(target):\n",
        "    return target\n",
        "  return tf.keras.utils.get_file(target, origin=url, extract=extract)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyYVJA6J8keb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colab_root = \"/content/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIL5DP2N75Rj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_path = os.path.join(colab_root, \"starter.csv\")\n",
        "download_if_missing(\"https://storage.googleapis.com/applied-dl/mini-vqa/starter.csv\",\n",
        "                     csv_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytd6Ldr5uWzX",
        "colab_type": "text"
      },
      "source": [
        "Tip. Here are a few shell commands you can use to quickly inspect the CSV and familiarize yourself with the format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxxqcmZUouZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head $csv_path -n3 # take a look at a few rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mihenkao0zF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wc -l $csv_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTaTlQBcvSna",
        "colab_type": "text"
      },
      "source": [
        "### Download the COCO training images with curl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33kwCeYPriJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Download the training images with curl, and unzip them.\n",
        "# If you're working in Colab, be mindful of disk usage."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjSgaJSsehJo",
        "colab_type": "text"
      },
      "source": [
        "### Get to know your data.\n",
        "Write code to display a few images, questions, and answers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g3dj9FGMOnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Next, familiarze yourself with the data. \n",
        "# Add code to display a few images from the training set using matplotlib,\n",
        "# and their corresponding questions and answers from the starter CSV.\n",
        "# Don't trust that your data is perfect (any bugs are unintentional \n",
        "# on my part, but there may well be some). The time you spend poking\n",
        "# around with it is always a good investment."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxJiIe11MfbY",
        "colab_type": "text"
      },
      "source": [
        "### As a sanity check, verify you can locate all the images\n",
        "Write code to verfy you can find every image mentioned in your starter CSV (in the unzipped COCO images)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MAQXQV8MQRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Next, write code to verfy you can find every image mentioned in the starter \n",
        "# CSV on disk. For example, you may want write a loop like this:\n",
        "# import csv\n",
        "# with open('starter.csv') as myfile:\n",
        "#    reader = csv.reader(myfile, delimiter=',')\n",
        "#    for row in reader:\n",
        "#       question, answer, image = row\n",
        "#       image = image.strip()\n",
        "#       assert os.path.exists(os.path.join(coco_images_folder, image))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MyAV2_CemZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "013oczYur0hn",
        "colab_type": "text"
      },
      "source": [
        "### Resize the images to 299x299\n",
        "Next, resize the images referenced in starter CSV to 299x299 using Pillow.\n",
        "\n",
        "### Tips\n",
        "Here is code you can use to create a thumbnail of an image.\n",
        "\n",
        "```\n",
        "size = 299, 299\n",
        "im = Image.open(path_to_image)\n",
        "im.thumbnail(size)\n",
        "out_file = os.path.join(your_directory, image_name)\n",
        "im.save(out_file, \"JPEG\")\n",
        "```\n",
        "\n",
        "Note: do not change the image names when saving (your rezied image names should exactly match the names in the starter code)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDj5cuUomwrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Resize all the images in the starter CSV to 299x299.\n",
        "# Display a few and make sure the resize is working as expected."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUTdysE3MzwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Delete all images not mentioned in the starter CSV.\n",
        "# As before (as a sanity check) make sure you can still locate every image \n",
        "# (now resized) mentioned in your starter CSV.\n",
        "# Your code here\n",
        "# Make sure your image folder contains only the images mentioned in your starter CSV."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QixVwkUs5TG",
        "colab_type": "text"
      },
      "source": [
        "### Create a zip of your resized images\n",
        "\n",
        "### Tips\n",
        "Here is code you can use to zip a directory, creating a new zip file \"thumbnails.zip\"\n",
        "\n",
        "```\n",
        "shutil.make_archive('thumbnails', 'zip', [directory_to_zip])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VmB8STGwUM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Create a thumbnails.zip\n",
        "# Verify the size is around 500MB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWsc2ECMu4oF",
        "colab_type": "text"
      },
      "source": [
        "### Upload your thumbnails.zip to the Cloud\n",
        "\n",
        "You want to host your thumbnails.zip file at a URL, for example: ```https://storage.googleapis.com/[your_bucket_name]/thumbnails.zip```, so you can easily download it in your second notebook.\n",
        "\n",
        "You can use either your Columbia account, or any Cloud provider.\n",
        "Google Cloud [storage buckets](https://cloud.google.com/storage/docs/creating-buckets) are fine for this. You can use the [free tier](https://cloud.google.com/free/).\n",
        "\n",
        "After this is done, you're ready to start on notebook two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKNgLhjovm5u",
        "colab_type": "text"
      },
      "source": [
        "## Notebook two: Preprocessing and training\n",
        "Create a new notebook for this. At the start, download your starter CSV and thumbnails.zip.\n",
        "\n",
        "The starter code below assumes you have created a zip file called \"thumbnails.zip\" containing only the resized images from the COOC training set mentioned in the starter CSV, and that you can download it from a URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbP0Mkw_wQyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colab_root = \"/content/\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzwWPJ5muPS4",
        "colab_type": "text"
      },
      "source": [
        "### Download the starter CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPGj4DpRwWE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_path = os.path.join(colab_root, \"starter.csv\")\n",
        "download_if_missing(\"https://storage.googleapis.com/applied-dl/mini-vqa/starter.csv\",\n",
        "                     csv_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoIeRieewcDj",
        "colab_type": "text"
      },
      "source": [
        "### Download and unzip your thumbnails\n",
        "\n",
        "This file is small enough that you can quickly download it with this method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mrz3G3CxO3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "YOUR_THUMBNAILS_URL = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgMG4Rav8H_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "thumbnails_file = os.path.join(colab_root, \"thumbnails.zip\")\n",
        "images_folder = os.path.join(colab_root, 'thumbnails/')\n",
        "download_if_missing(YOUR_THUMBNAILS_URL,\n",
        "                    thumbnails_file,\n",
        "                    extract=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Manup0SxRKF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q $thumbnails_file -d $images_folder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9kw6tdtSHdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls $images_folder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifotaRuEyPhR",
        "colab_type": "text"
      },
      "source": [
        "### Sanity check\n",
        "\n",
        "At this point, you have all the data you need to train your model (questions, answers, and images) in a convenient and easy to read format. \n",
        "\n",
        "Now would be a good time for another santity check. Write a block of code to iterate over the starter CSV file. For each image, make sure you can locate it on disk. E.g., \n",
        "\n",
        "``` \n",
        "for question, answer, image_name in data:\n",
        "  img_path = os.path.join(images_folder, image_name)\n",
        "  assert os.path.exists(img_path)\n",
        "```\n",
        "\n",
        "Since we're in a new notebook, again display a few questions, answers, and images - verify everything works as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyqY8UNsxmK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP0BexdF8NXH",
        "colab_type": "text"
      },
      "source": [
        "### Convert answers to numeric format\n",
        "\n",
        "The answers in the starter CSV are \"yes\" or \"no\". For convenience, now would be a good time to change those to 1.0 and 0.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsvJHe1krz9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Convert the answers column in the starter CSV to 1.0 and 0.0\n",
        "# You do not need to update the CSV itself. If you have a data structure\n",
        "# in memory, you can work with that instead."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_pGuZWXx4vE",
        "colab_type": "text"
      },
      "source": [
        "### Convert image names to absolute paths\n",
        "\n",
        "Next, for convenience, it may be helpful to update the images column in the starter CSV from filenames ```COCO_train2014_000000320111.jpg``` to absolute paths ```/content/images/COCO_train2014_000000320111.jpg```. This will save you some code down the road when it comes time to open them up (you won't need to worry about the relationship between where the starter CSV file is stored, and your images folder)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9lbPJyvxv6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Convert the image filenames in the starter CSV to absolute paths\n",
        "# You do not need to update the CSV itself. If you have a data structure\n",
        "# in memory, you can work with that instead."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqWvyk_54MEB",
        "colab_type": "text"
      },
      "source": [
        "### Shuffle and create a test set\n",
        "For this assignment, you do not need to download the COCO validation or test sets. Instead, shuffle your starter CSV, and use some of the rows as a test set (say, 4,000 rows). Move them to a separate CSV or data structure, and revisit them later. I realize some of the same images may appear in the train and test set (although with different questions). For this assignment, that's okay. If you prefer, you can write code to ensure the test set has unique images that do not appear in the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUeKcg8H4OH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Shuffle the starter CSV\n",
        "# Remove 4,000 rows (or so) and move them to a separate file or data structure"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRCqCG4eNWzX",
        "colab_type": "text"
      },
      "source": [
        "### Limit the size of the dataset, and balance.\n",
        "\n",
        "The dataset is fairly large. Let's simplify and start small. Instead of working with +/-100,000 questions - a better number would be about 20,000 questions.\n",
        "\n",
        "As a suggestion, you may also want to balance the data (so you have an equal number of positive and negative questions), just to make it a bit easier to debug your model and verify it's training as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcH6-VCnwmSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Limit the size of the starter CSV to 20,000 rows\n",
        "# Balance the data (so you have an equal number of \"yes\" and \"no\" answers) \n",
        "# in your 20,000 rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmSXbHqrfdFJ",
        "colab_type": "text"
      },
      "source": [
        "### Create training and validation sets\n",
        "You may want to do an 80:20 split on your balanced training set, giving you 16,000 training rows, and 4,000 validation rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMto3Q1rxcQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# The scikit-learn utilities (train_test_split) are your friend"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od0Q5SzEzd8C",
        "colab_type": "text"
      },
      "source": [
        "### Verify your train and validation sets look as expected\n",
        "You cannot spend too much time exploring the data. When I'm developing code for something like this on my own, I often run methods to verify that these splits have the number of rows I expect, to display a few images from each, and to see stats on the class balance. I also write code to verify the splits contain *only* yes/no answers. You may be tired of coding defensively at this point, but I assure you (especially when working with new and increasingly complicated datasets, this effort **always** pays off)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rXdrUZQz5f4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ITEBFcQRqdm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Forward your images through InceptionV3, and cache activations to disk\n",
        "\n",
        "Rather than training a CNN from scratch for your VQA model, you'll begin by using activations from a pretrained model.  \n",
        "\n",
        "* Instead of forward each image repeatedly through the model while training (which will be slow), let's do that once and save the activations to disk. \n",
        "\n",
        "* We're saving them to disk (rather than keeping them in memory) to accomodate different sizes of datasets down the road.\n",
        "\n",
        "I've written a good deal of this code for you, but you should go through it and carefully understand how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbMdRBX8TY5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a feature extraction model.\n",
        "# You should not need to modify this (though you may, if you'd like\n",
        "# to use a model other than Inception).\n",
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW0drXDAQu3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A method to load an image off disk, and extract activations using \n",
        "# the model above. You should not need to modify this.\n",
        "def image_to_activations(image_path):\n",
        "  img = tf.io.read_file(image_path)\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  img = tf.image.resize(img, (299, 299))\n",
        "  activations = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "  return activations, image_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2_1Ili7y3Fj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "\n",
        "# You'll need to extract activations for every image in your train, validation,\n",
        "# and test set. First, create a set of the absolute paths to all of these images \n",
        "# (image_path_set). Populate this with the absolute paths to all these images.\n",
        "image_path_set = set()\n",
        "\n",
        "\n",
        "print(\"Images to preprocess\", len(image_path_set))\n",
        "print(\"This make take a few minutes\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb9XjL8QWzuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This cell will extract activations for each image and save them to disk \n",
        "# in NumPy format. You should not need to modify this.\n",
        "\n",
        "# Note: we're not saving these activations to the cloud,\n",
        "# but you certainly could if you wanted to skip this step \n",
        "# in the future. If you look into doing that, it's best to save one large\n",
        "# zip with the activations, and download that and extract it locally\n",
        "# (rather than doing a bunch of network access to retrieve individual files),\n",
        "# especially when training your model.\n",
        "\n",
        "# Create a dataset to load each image off disk, and extract activations\n",
        "activation_dataset = tf.data.Dataset.from_tensor_slices(list(image_path_set))\n",
        "activation_dataset = activation_dataset.map(\n",
        "  image_to_activations, \n",
        "  num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(32)\n",
        "\n",
        "# Save all activations to disk in NumPy format\n",
        "for img_batch, path_batch in image_dataset:\n",
        "  batch_features = image_features_extract_model(img_batch)\n",
        "  for bf, p in zip(batch_features, path_batch):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Rb4DsORTui",
        "colab_type": "text"
      },
      "source": [
        "### Create lists of questions, answers, and images for your train, validation, and test set\n",
        "\n",
        "At this point, you may be reading your starter CSV directly from disk, or you may have your own data structure in memory. Since many of the methods we'll call from this point forward take lists as input, you may find it helpful to create a data structure with three lists, that correspond to the columns from your starter CSV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoIgQUwkR3PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here. Populate these for your training set.\n",
        "# questions_train = [] # a list of absolute paths to images in your training set\n",
        "# answers_train = [] # a list of questions in your training set\n",
        "# images_train = [] # a list of answers (in numeric format) in your training set\n",
        "\n",
        "# The order of these lists should match (e.g. the question, answer, and image \n",
        "# from row i of your train split of the starter CSV should be \n",
        "# questions_train[i], answers_train[i], images_train[i])."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLtOogc3R3bW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here. Populate these for your validation set.\n",
        "# questions_val = [] \n",
        "# answers_val = []\n",
        "# images_val = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arUBA1CcR3lS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here. Populate these for your test set\n",
        "# questions_test = []\n",
        "# answers_test = []\n",
        "# images_test = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTkz5C9eb1Ic",
        "colab_type": "text"
      },
      "source": [
        "### Create and fit a tokenizer\n",
        "\n",
        "Your model will use a LSTM to process the questions. First, you'll need to vectorize your text. Tokenize the questions, and limit the vocabulary size to a reasonable size (for example, the top 3,000 words). A larger size will make a higher accuracy possible, but complicate and slow your model. Reminder: fit the tokenizer only on the training questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYElFeEKxE3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "VOCAB_SIZE = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4myZY6zxDwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You should not need to modify this code\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(questions_train)\n",
        "\n",
        "# Note, the tokenizer's word_index will not respect VOCAB_SIZE.\n",
        "# but, that parameter will be respected in later methods,\n",
        "# (for example, when you call text_to_sequences).\n",
        "# Also note that '0' is a reserved index for padding.\n",
        "print(\"Word index\", len(tokenizer.word_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gvj1ZE-fTrP",
        "colab_type": "text"
      },
      "source": [
        "### Vectorize the questions\n",
        "In this section, you will use your tokenizer to vectorize the questions using ```texts_to_sequences```. For an example of texts_to_sequences, see this [notebook](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-one-hot-encoding-of-words-or-characters.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhQu6yZhfi7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Use the texts_to_sequences utility to vectorize your training, \n",
        "# validation, and test questions. \n",
        "\n",
        "# To follow along with the starter code below, you should use\n",
        "# these naming conventions\n",
        "\n",
        "# sequences_train = tokenizer.texts_to_sequences(questions_train)\n",
        "# sequences_val = ...\n",
        "# sequences_test = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiDIdjpY6FHO",
        "colab_type": "text"
      },
      "source": [
        "### Pad sequences\n",
        "\n",
        "In this section, you will pad the vectorized questions using ```pad_sequences```. Your maximum sequence length is a design decision, just like vocab size. Advice, start with something short, so your model trains faster (maybe, between 10 and 20).\n",
        "\n",
        "For an example of pad_sequences, see this [notebook](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.2-understanding-recurrent-neural-networks.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7KqB0-t0vci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# To choose a reasonable sequence length, examine the length of all the \n",
        "# tokenized questions in the training set (in words).\n",
        "# Justify your choice, e.g. you should note that percentage of the \n",
        "# training questions that fall under that length (and will not be trimmed), \n",
        "# and the number of questions that will be."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqgmSiLzeY41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "MAX_SEQ_LEN = None "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k66XWHrPdlKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Use the pad_sequences utility to pad your training, \n",
        "# validation, and test questions.\n",
        "\n",
        "# To follow along with the rest of the statter code, you can use these\n",
        "# naming conventions\n",
        "\n",
        "# padded_train = tf.keras.preprocessing.sequence.pad_sequences...\n",
        "# padded_val = ...\n",
        "# paddeds_test = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCKOXMDwTvtE",
        "colab_type": "text"
      },
      "source": [
        "### Sanity check\n",
        "You've just done a **bunch** of preprocessing. Optionally, now would be a good time to write a block of code to verify the tokenized and padded sequences are in the format you expect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp9Wwup-T3aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here (optional).\n",
        "# Since you have a word_index from the tokenizer, you may \n",
        "# also want to write code to convert from a vectorized question\n",
        "# back to a string."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wocaXqRkZa_",
        "colab_type": "text"
      },
      "source": [
        "### Create a tf.dataset for training, validation, and testing\n",
        "\n",
        "The method to create the dataset is provided for you, though you will need to get it working by passing the ```padded_train```, ```answers_train``` ```and images_train``` lists you created above.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK46QgJ7fsiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You should not need to modify this.\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HoznUX469tT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You should not need to modify this.\n",
        "# Load cached activations off disk.\n",
        "def load_np(img_path, question, answer):\n",
        "  activations = np.load(img_path.decode('utf-8')+'.npy')\n",
        "  return activations, question, answer, img_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS1JpUNV65cE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You should not need to modify this.\n",
        "\n",
        "# This method will create a dataset that returns four elements.\n",
        "# - a batch of activations (loaded from disk)\n",
        "# - a batch of padded questions\n",
        "# - a batch of numeric answers\n",
        "# - a batch of absolute paths to the corresponding images\n",
        "def create_dataset(images, sequences, answers):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((images, \n",
        "                                                sequences, \n",
        "                                                answers))\n",
        "  # TODO jbgordon@: rewrite this to be clearer\n",
        "  # Load the cached activations off disk\n",
        "  dataset = dataset.map(lambda x, y, z: tf.numpy_function(\n",
        "      load_np, [x, y, z], [tf.float32, tf.int32, tf.float32, tf.string]),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  # Shuffle and batch\n",
        "  dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAr49L-1D0pW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "\n",
        "# Call the above method to create train, val, and test datasets.\n",
        "# If you want to follow along with the starter code, I suggest these\n",
        "# variable names:\n",
        "\n",
        "# train_ds = create_dataset(images_train, padded_train, answers_train)\n",
        "# val_ds = ...\n",
        "# test_ds = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA3D5qNfVWSs",
        "colab_type": "text"
      },
      "source": [
        "### Sanity check (optional)\n",
        "That dataset creation method is complicated. Write a block of code that demonstrates how to use the dataset (e.g., retrieve a batch of activations, questions, answeres, and images paths) and verify they look as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS0KjlYM7UKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here (optional)\n",
        "# Verify your datasets are working properly\n",
        "\n",
        "# Here is code you can use to quickly retrieve a batch of data\n",
        "\n",
        "# my_iterator = iter(train_ds)\n",
        "# activations_batch, questions_batch, answers_batch, paths_batch = next(my_iterator)\n",
        "\n",
        "# print(activations_batch.shape, \n",
        "#       questions_batch.shape, \n",
        "#       answers_batch.shape, \n",
        "#       paths_batch.shape)\n",
        "\n",
        "# print(activations_batch, \n",
        "#       questions_batch, \n",
        "#       answers_batch, \n",
        "#       paths_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z7a9VExmj_5",
        "colab_type": "text"
      },
      "source": [
        "### Define your VQA model\n",
        "\n",
        "You can base your code off the example give toward bottom of this [guide](https://keras.io/getting-started/functional-api-guide/) (but use a smaller model to start). Aim for a couple hundred thousand parameters or so"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIjyol_olOxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, Input, LSTM\n",
        "from tensorflow.keras.models import Model, Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUOdkiOemwPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Below is starter code for your model for you to complete.\n",
        "# See https://keras.io/getting-started/functional-api-guide/ for the idea.\n",
        "# The vision model is written for you. You will need to write the question\n",
        "# model.\n",
        "\n",
        "# Input to your vision model (activations from Inception-V3,\n",
        "# loaded off disk disk by the dataset above).\n",
        "image_input = Input(shape=(8, 8, 2048)) \n",
        "vision_model = Sequential()\n",
        "# Used to reduce the number of parameters (rather using a dense layer here).\n",
        "vision_model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "# Output of your vision model\n",
        "encoded_image = vision_model(image_input) \n",
        "\n",
        "# Your code here\n",
        "# Write your test processing model that takes the vectorized and padded\n",
        "# question as input.\n",
        "# As in the guide above, you will want to produce an `encoded_question`\n",
        "# as output\n",
        "question_input = None # your code\n",
        "embedded_question = None # your code\n",
        "encoded_question = None # your code\n",
        "\n",
        "# Concatenate the encoded image and question\n",
        "merged = tf.keras.layers.concatenate([encoded_image, encoded_question])\n",
        "\n",
        "# Optionally, add a small dense layer\n",
        "# ...\n",
        "\n",
        "# Next, add a binary classifier on top\n",
        "output = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "# Your final model\n",
        "model = Model(inputs=[image_input, question_input], outputs=output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc5OEmXsp4CH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XcTtLlPoHjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cITtsogNXvgp",
        "colab_type": "text"
      },
      "source": [
        "### Plot your model\n",
        "Create a schematic that shows the graph of your model, using [plot_model](https://keras.io/visualization/). This can be helpful to ensure the vision and question paths look at expected (it's also super cool, and more informative than .summary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPYyzzf2XxbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You should not need to modify this.\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WytdlCrO6iaq",
        "colab_type": "text"
      },
      "source": [
        "### Fit your model on a single batch\n",
        "Before training on your entire dataset, a helpful first step is to train repeatedly on a single batch, and verify the loss goes to zero. If your model is working properly, it should be able to memorize a batch of data. We will use ```model.train_on_batch``` for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvpBQv9Z9HlH",
        "colab": {}
      },
      "source": [
        "# You should not nee to modify this.\n",
        "# Retrieve a batch of data from your train dataset\n",
        "activations_batch, questions_batch, answers_batch, paths_batch = next(iter(train_ds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE-qMn5dgeYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "\n",
        "# Train them model repeatedly using model.train_on_batch\n",
        "# Verify the loss goes to zero after +/- 100 training steps.\n",
        "# If it does not, now would be a great time to debug \n",
        "# before proceeding further.\n",
        "\n",
        "# e.g.\n",
        "# metrics = model.train_on_batch ....\n",
        "# metrics is a list (loss is metrics[0], accuracy is metrics[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02reDBJKtCk8",
        "colab_type": "text"
      },
      "source": [
        "### Use your model to make predictions on the same batch above\n",
        "Compare the predicted answer to the label. Verify they match."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaIEX9AJnRTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You should not need to modify this, but you will want to \n",
        "# carefully inspect the output.\n",
        "for prediction, answer in zip(model.predict(x=[activations_batch, questions_batch]), answers_batch):\n",
        "  print(prediction, answer.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDqSu9Ym04nR",
        "colab_type": "text"
      },
      "source": [
        "### Create a checkpoints directory\n",
        "\n",
        "Once you have been able to train your model on a single batch, it's time to begin training on your training dataset. It may take a while to train for a single epoch, and it would be unfortunate if Colab disconnected after training for a while, and you lost your progress.\n",
        "\n",
        "* After each training epoch, save the model's weights in a checkpoint file. You can learn more about how to create checkpoints [here](https://www.tensorflow.org/guide/keras/save_and_serialize). \n",
        "\n",
        "* To resume training, load the latest checkpoint from disk. This will restore the latest weights and resume your progress. If it does not exist, begin training from scratch.\n",
        "\n",
        "* As a tip, you may want to store your checkpoints in Google Drive, so you'll still have access to them if Colab disconnects. \n",
        "\n",
        "Note that running long jobs is not what Colab is intended for. Normally, you could simple save your checkpoints on the machine you're working on. Saving to drive adds an extra step, but it's worthwhile learning how to do.\n",
        "\n",
        "Tip: after mounting Drive, **do not** programmatically run any commands to delete files from your checkpoints folder or elsewhere (e.g., by using ```!rm -rf```) in Colab. If you're not careful, you may accidentally wipe out your entire drive if you make a programming mistake. \n",
        "\n",
        "* Instead, if you need to delete checkpoints, do so manually through the drive user interface. Note that files deleted through the UI may take a minute or so to \"actually\" be deleted as reflected by ```!ls``` commands run from Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DhXkN3i06TY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount drive\n",
        "drive.mount('/gdrive')\n",
        "drive_root = '/gdrive/My Drive/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMzFtWcJy8eC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you'd like to save checkpoints in drive, you will need to uncomment\n",
        "# the code below. Alternatively, you can modify it to save\n",
        "# checkpoints in Colab (these will not persist if your instance is terminated,\n",
        "# but you can manually download them if you like).\n",
        "checkpoint_dir = os.path.join(drive_root, \"checkpoints\")\n",
        "checkpoint_dir = os.path.join(checkpoint_dir, \"hw4\")\n",
        "\n",
        "# Used for formatting\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"cp-{epoch:08d}.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcLFRePmatah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Uncomment this if you'd like to create a checkpoints folder in your drive\n",
        "# print(\"Checkpoints directory is\", checkpoint_dir)\n",
        "# if os.path.exists(checkpoint_dir):\n",
        "#   print(\"Checkpoints folder already exists\")\n",
        "# else:\n",
        "#   print(\"Creating a checkpoints directory\")\n",
        "#   os.makedirs(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF_e7Sf1awnH",
        "colab_type": "text"
      },
      "source": [
        "### Restore the latest checkpoint (if it exists)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCDBhc4XnnGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You should not need to modify this code.\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "if latest != None:\n",
        "  print(\"Loading weights from\", latest)\n",
        "  model.load_weights(latest)\n",
        "else:\n",
        "  print(\"Checkpoint not found. Starting from scratch\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVCezYfDa6n_",
        "colab_type": "text"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRVbkMtR_lp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilities to help us record metrics.\n",
        "# You should not need to modify this code\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "\n",
        "val_loss = tf.keras.metrics.BinaryCrossentropy(name='val_loss')\n",
        "val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PahaAZPgu-hV",
        "colab_type": "text"
      },
      "source": [
        "### Code to evaluate on the validation dataset\n",
        "The validation dataset may be large. It would be wasteful to evaluate on the entire validation dataset each training epoch. Instead, you could evaluate every N epochs, or, you can use the below methoid to evaluate for a fixed number of steps (batches). This will give you a noisier evaluation, but a useful indicator of how your model is doing over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mtca2G8FCF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You should not need to modify this.\n",
        "def evaluate(max_steps=None):\n",
        "  steps = 0\n",
        "  for activation_batch, question_batch, answer_batch, path_batch in val_ds:\n",
        "    if max_steps != None and steps == max_steps:\n",
        "      break\n",
        "    predictions = model.predict(x=[activation_batch, question_batch])\n",
        "    steps += 1 \n",
        "    # Record metrics after each batch\n",
        "    val_loss(answer_batch, predictions)\n",
        "    val_accuracy(answer_batch, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpRR3fbb03z3",
        "colab_type": "text"
      },
      "source": [
        "### Train the model\n",
        "As before, we'll write our training loop using ```train_on_batch```. This is intermediate in complexity between using ```.fit``` and writing everything from scratch using a ```GradientTape```. Because we're not using ```.fit```. Since we're not using .fit, there's a bit of extra code we need to write ourselves to track loss and accuracy as we go."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e-1dK1YCR8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Complete this cell (there are only a few parts to write)\n",
        "\n",
        "# Used to track loss and accuracy as we go\n",
        "# You should not need to modify these\n",
        "train_loss_history, train_acc_history = [], []\n",
        "val_loss_history, val_acc_history = [], []\n",
        "\n",
        "epochs = None # Your code here\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  # Train for one epoch\n",
        "  for activation_batch, question_batch, answer_batch, path_batch in train_ds:\n",
        "    result = model.train_on_batch(x=[activation_batch, question_batch], y=answer_batch)\n",
        "\n",
        "    # Record metrics after each batch\n",
        "    train_loss(result[0])\n",
        "    train_accuracy(result[1])\n",
        "\n",
        "  # Evaluate for a few steps\n",
        "  evaluate(max_steps=100)\n",
        "\n",
        "  # Print progress\n",
        "  # You should not need to modify this.\n",
        "  template = 'Epoch {}, Loss: {:.2f}, Accuracy: {:.2f}, Val Loss {:.2f}, Val Accuracy {:.2f}, Time: {:.1f} secs'\n",
        "  print(template.format(epoch,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result() * 100,\n",
        "                        val_loss.result(),\n",
        "                        val_accuracy.result() * 100,\n",
        "                        time.time() - start))\n",
        "  \n",
        "  # Record history\n",
        "  train_loss_history.append(train_loss.result())\n",
        "  train_acc_history.append(train_accuracy.result() * 100)\n",
        "  val_loss_history.append(val_loss.result())\n",
        "  val_acc_history.append(val_accuracy.result() * 100)\n",
        "\n",
        "  # Reset the metrics for the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  val_loss.reset_states()\n",
        "  val_accuracy.reset_states()\n",
        "\n",
        "  # Your code here\n",
        "  # Save a checkpoint after each epoch\n",
        "  print(\"Saving weights\")\n",
        "  # model.save_weights ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IQd0z3RwfIN",
        "colab_type": "text"
      },
      "source": [
        "### Create plots of your training and validation loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_UgdSUaoGND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzTUcK3WclOi",
        "colab_type": "text"
      },
      "source": [
        "### At this point, you have end-to-end code to train VQA model\n",
        "Now you can begin working on increasing accuracy. For this assignment, your model should at least be able to fit your training split reasonably well. You do not need to build a model that performs well on validation to receive full credit (I recently made the starter dataset a bit more difficult, as a result - you may need to train using more data than I want you to do for a homework assignment)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCe18LhedPsg",
        "colab_type": "text"
      },
      "source": [
        "### Run one experiment to improve accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujqBdK1rdEbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code and brief writeup here\n",
        "# Think about how to improve the accuracy of your model, \n",
        "# either on the training or validation set.\n",
        "# Design and run an experiment to do so (by writting whatever code is necessary)\n",
        "# Include the code from your experiment here, and a few notes on the results\n",
        "# (one paragraph is fine)."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YisAhTTCHxd_",
        "colab_type": "text"
      },
      "source": [
        "### Finally, evaluate your model on the test set\n",
        "How well did it do? This assignment is lengthy. For our purposes, it's fine to report the accuracy - and you're done :) \n",
        "\n",
        "For a proper evaluation, take a look at this [paper](https://arxiv.org/abs/1612.00837), as discussed in class.\n",
        "\n",
        "Note: if your model is performing poorly on the test set, you can still receive full credit for this assignment as mentioned above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSQkgWG8Hyfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Restore the latest checkpoint\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "assert latest != None\n",
        "model.load_weights(latest)\n",
        "print(\"loaded weights from\", latest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJhBzTEfxHLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "# Calculate accuracy on the test set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQZtYaVO5Dhe",
        "colab_type": "text"
      },
      "source": [
        "That's it! This was a long assignment, I hope it was a useful (and fun!) learning experience."
      ]
    }
  ]
}