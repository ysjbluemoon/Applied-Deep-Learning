{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nG2GQRD78oOd"
   },
   "source": [
    "### y = m * x + b\n",
    "\n",
    "A minimal example of linear regression in TensorFlow 2.0, written from scratch without using any built-in layers, optimizers, or loss functions. We'll create a few points on a scatter plot, then find the best fit line using the equation `y = m * x + b`.\n",
    "\n",
    "### Bonus\n",
    "When we're finished, we'll plot the error surface as a function of m and b, to visualize the gradient descent process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Z7ZBuk95jCb"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8doDs0i5oOU"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2n5vd8i5yCg"
   },
   "source": [
    "Create a noisy dataset that's roughly linear, according to the equation y = m * x + b + noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mekZMKUW5xQj"
   },
   "outputs": [],
   "source": [
    "def make_noisy_data(m=0.1, b=0.3, n=100):\n",
    "  x = tf.random.uniform(shape=(n,))\n",
    "  noise = tf.random.normal(shape=(len(x),), stddev=0.01)\n",
    "  y = m * x + b + noise\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jsb2XBE5TPNY"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = make_noisy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tSNaq8W52A2"
   },
   "outputs": [],
   "source": [
    "plt.plot(x_train, y_train, 'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zpzVrkNhI3y5"
   },
   "source": [
    "Define variables for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mwoChE9N5-aE"
   },
   "outputs": [],
   "source": [
    "m = tf.Variable(0.)\n",
    "b = tf.Variable(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0SEZq4GhJV5d"
   },
   "source": [
    "Predict y given x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TslVztFY7uQT"
   },
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "  y = m * x + b\n",
    "  return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cJW-r9JEJi1i"
   },
   "source": [
    "Our loss will be the squared difference between the predicted values and the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTmFCjZX7JGV"
   },
   "outputs": [],
   "source": [
    "def squared_error(y_pred, y_true):\n",
    "  return tf.reduce_mean(tf.square(y_pred - y_true)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WVDFLZBrBikA"
   },
   "source": [
    "Calculate loss before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "25EhTz5QBkYS"
   },
   "outputs": [],
   "source": [
    "loss = squared_error(predict(x_train), y_train)\n",
    "print(\"Starting loss\", loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WjHGdac1EWj6"
   },
   "source": [
    "Use gradient descent to gradually improve our guess for `m` and `b`. At each step, we'll nudge them a little bit in the right direction to reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qp6QFVQD6xB_"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "steps = 200\n",
    "\n",
    "for i in range(steps):\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = predict(x_train)\n",
    "    loss = squared_error(predictions, y_train)\n",
    "    \n",
    "  gradients = tape.gradient(loss, [m, b])\n",
    "  \n",
    "  m.assign_sub(gradients[0] * learning_rate)\n",
    "  b.assign_sub(gradients[1] * learning_rate)\n",
    "  \n",
    "  if i % 20 == 0:    \n",
    "    print(\"Step %d, Loss %f\" % (i, loss.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4lPk7ZC87kU"
   },
   "source": [
    "The learned values for m and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qVsdtuyp6-aP"
   },
   "outputs": [],
   "source": [
    "print (\"m: %f, b: %f\" % (m.numpy(), b.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ra6bq2rV_19Z"
   },
   "source": [
    "Plot the best fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rBvBWPgO_3wy"
   },
   "outputs": [],
   "source": [
    "plt.plot(x_train, y_train, 'b.')\n",
    "plt.plot(x_train, predict(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIikRd5kDQSp"
   },
   "source": [
    "A couple things you can explore:\n",
    "\n",
    "* To understand gradient descent, try printing out the `gradients` calculated below. See how they're used to adjust the variables (`m` and `b`).\n",
    "\n",
    "* You can use TF 2.0 a lot like NumPy.  Try printing out the training data we created (`x_train`, `y_train`) and understand the format. Next, do the same for the variables (m and b). Notice both of these can be converted to NumPy format (with `.numpy()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJ17StfK6-pC"
   },
   "source": [
    "### Bonus\n",
    "Let's visualize the error surface. This section is included purely for fun, you can skip it without missing anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEQ_D8iP7fkf"
   },
   "outputs": [],
   "source": [
    "# Warning: hacky code ahead\n",
    "\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# To plot the error surface, we'll need to get the loss\n",
    "# for a bunch of different values for m and b.\n",
    "\n",
    "ms = np.linspace(-1, 1)\n",
    "bs = np.linspace(-1, 1)\n",
    "m_mesh, b_mesh = np.meshgrid(ms, bs)\n",
    "\n",
    "def loss_for_values(m, b):\n",
    "  y = m * x_train + b\n",
    "  loss = squared_error(y, y_train)\n",
    "  return loss\n",
    "\n",
    "zs = np.array([loss_for_values(m, b) for (m,b) in zip(np.ravel(m_mesh), \n",
    "                                                      np.ravel(b_mesh))])\n",
    "z_mesh = zs.reshape(m_mesh.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(m_mesh, b_mesh, z_mesh, color='b', alpha=0.06)\n",
    "\n",
    "# At this point we have an error surface. \n",
    "# Now we'll need a history of the gradient descent steps.\n",
    "# So as not to complicate the above code,\n",
    "# let's retrain the model here, keeping\n",
    "# track of m, b, and loss at each step.\n",
    "\n",
    "# Intentionally start with this guess to \n",
    "# make the plot nicer\n",
    "m = tf.Variable(-.5)\n",
    "b = tf.Variable(-.75)\n",
    "\n",
    "history = []\n",
    "\n",
    "for i in range(steps):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = predict(x_train)\n",
    "    loss = squared_error(predictions, y_train)\n",
    "  gradients = tape.gradient(loss, [m, b])\n",
    "  history.append((m.numpy(), b.numpy(), loss.numpy()))\n",
    "  m.assign_sub(gradients[0] * learning_rate)\n",
    "  b.assign_sub(gradients[1] * learning_rate)\n",
    "\n",
    "# Plot the trajectory\n",
    "ax.plot([h[0] for h in history], \n",
    "        [h[1] for h in history], \n",
    "        [h[2] for h in history],\n",
    "        marker='o')\n",
    "\n",
    "ax.set_xlabel('m', fontsize=18, labelpad=20)\n",
    "ax.set_ylabel('b', fontsize=18, labelpad=20)\n",
    "ax.set_zlabel('loss', fontsize=18, labelpad=20)\n",
    "\n",
    "ax.view_init(elev=22, azim=28)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "linear-regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
